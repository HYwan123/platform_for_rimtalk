from fastapi import FastAPI, HTTPException, Request
from fastapi.responses import StreamingResponse, JSONResponse
from pydantic import BaseModel, Field
from openai import AsyncOpenAI
import uvicorn
from typing import List, Optional, Literal, Union

# --- 1. OpenaiClient ä¿æŒä¸å˜ ---
class OpenaiClient:
    """
    å•ä¾‹æ¨¡å¼çš„ OpenAI å¼‚æ­¥å®¢æˆ·ç«¯ï¼Œç”¨äºè¿æ¥æ‚¨çš„ iflow API æœåŠ¡ã€‚
    """
    _instance = None
    conn = AsyncOpenAI(
        base_url='https://apis.iflow.cn/v1',
        timeout=999999999,
        # æ³¨æ„: å®é™…éƒ¨ç½²æ—¶ï¼ŒAPI Key åº”è¯¥ä»ç¯å¢ƒå˜é‡æˆ–å®‰å…¨é…ç½®ä¸­è·å–
        api_key='sk-d5a9b4f92ce356963a48f06322867811' 
    )

    def __new__(cls):
        if not cls._instance:
            cls._instance = super().__new__(cls)
        return cls._instance

    @classmethod
    async def get_client(cls):
        return cls.conn

# --- 2. Pydantic è¯·æ±‚æ¨¡å‹ (å…¼å®¹ OpenAI API) ---
class Message(BaseModel):
    """å®šä¹‰æ¶ˆæ¯ç»“æ„"""
    role: Literal["system", "user", "assistant", "tool"]
    content: str

class ChatRequest(BaseModel):
    """å®šä¹‰èŠå¤©å®Œæˆè¯·æ±‚ä½“"""
    model: str = Field(..., description="è¦ä½¿ç”¨çš„æ¨¡å‹åç§°ï¼Œå¦‚ 'Qwen3-Max'")
    messages: List[Message] = Field(..., description="å¯¹è¯å†å²æ¶ˆæ¯åˆ—è¡¨")
    stream: bool = Field(False, description="æ˜¯å¦ä»¥æµå¼æ–¹å¼è¿”å›å“åº”")
    temperature: Optional[float] = Field(None, ge=0.0, le=2.0)
    top_p: Optional[float] = Field(None, ge=0.0, le=1.0)
    # è¿˜å¯ä»¥æ·»åŠ  max_tokens, stop ç­‰å…¶ä»–å‚æ•°

# --- 3. FastAPI åº”ç”¨å’Œè·¯ç”± ---
app = FastAPI(title="Ollama/OpenAI API Emulator")

# å®ä¾‹åŒ–å®¢æˆ·ç«¯ï¼ˆå•ä¾‹ï¼‰
openai_client = OpenaiClient()


from fastapi import FastAPI, HTTPException
from fastapi.responses import Response # ä½¿ç”¨ Response æ›¿ä»£ JSONResponse
import json # éœ€è¦å¯¼å…¥å†…ç½®çš„ json åº“
# ... (å…¶ä»–å¯¼å…¥ä¿æŒä¸å˜) ...



@app.post('/api/chat')
async def chat_completions(request: ChatRequest):
    conn = await openai_client.get_client()

    messages_param = [m.model_dump(exclude_none=True) for m in request.messages]

    async def event_generator():
        stream = await conn.chat.completions.create(
            model=request.model,
            messages=messages_param, # type: ignore
            stream=True
        ) # type: ignore
        async for chunk in stream:
            json_str = chunk.model_dump_json()
            # æ›¿æ¢ä¸­æ–‡å¼•å·
            json_str = json_str.replace('â€œ', '"').replace('â€', '"')
            yield f"data: {json_str}\n\n"

    return StreamingResponse(event_generator(), media_type="text/event-stream")



# --- 4. å¯åŠ¨å‡½æ•° ---
def run_server() -> None:
    """å¯åŠ¨ uvicorn æœåŠ¡å™¨"""
    print("ğŸš€ API æ¨¡æ‹ŸæœåŠ¡æ­£åœ¨å¯åŠ¨...")
    uvicorn.run(app, host="0.0.0.0", port=15432)

if __name__ == '__main__':
    run_server()
