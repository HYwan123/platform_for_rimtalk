from fastapi import FastAPI, HTTPException, Request
from fastapi.responses import StreamingResponse, JSONResponse
from app.utils.openai_client import OpenaiClient
import uvicorn
from app.model.chat import *
from app.utils.to_chunk import *


# --- 3. FastAPI åº”ç”¨å’Œè·¯ç”± ---
app = FastAPI(title="Ollama/OpenAI API Emulator")

# å®ä¾‹åŒ–å®¢æˆ·ç«¯ï¼ˆå•ä¾‹ï¼‰ 
openai_client = OpenaiClient()




@app.post('/api/chat')
async def chat_completions(request: ChatRequest):
    conn = await openai_client.get_client()

    messages_param = [m.model_dump(exclude_none=True) for m in request.messages]


    response = await conn.chat.completions.create(
        model=request.model,
        messages=messages_param, # type: ignore
        stream=False,
        response_format=
        {
            "type": "json_schema",
            "schema": OutputMessage.model_json_schema()
        } # type: ignore
    ) # type: ignore
    choice = response.choices[0]

    if hasattr(choice, "message") and hasattr(choice.message, "content"):
        print(choice.message.content)
    else:
        print(response)

    return StreamingResponse(event_generator(response) ,media_type="text/event-stream")

"""


"""

# --- 4. å¯åŠ¨å‡½æ•° ---
def run_server() -> None:
    """å¯åŠ¨ uvicorn æœåŠ¡å™¨"""
    print("ğŸš€ API æ¨¡æ‹ŸæœåŠ¡æ­£åœ¨å¯åŠ¨...")
    uvicorn.run(app, host="0.0.0.0", port=15432)

if __name__ == '__main__':
    run_server()
